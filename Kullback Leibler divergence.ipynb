{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kullback Leibler Divergence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$1$. Expectation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$2$.Information Content (IC)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$3$.Entropy of a Random Variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$4$.Information Content for Number of Bits Required to Transmit a Massage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " $5$.Cross Entropy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$6$. KL-Divergence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 . Expectation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's take an example and try to understand this concept."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now we take four cricket teams which are given by ${A}$,${B}$,${C}$,${D}$ is a kinds of random variabels and due to there past data the winning probability associated with each four random varaibles is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "X = {A,B,C,D}\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "P(X) = [0.4,0.2,0.1,0.3]\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "there is illegal betting on given team and what price is providing by individual team is given below. which we can represented by ${G}$. stand for ${Gain}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "G(X) = [10000 , 20000,-8000,5000]\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "so now expected profit is given by probability associated with random variabels and there corresponding ${Gain}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "Exprected-profit(X) = (0.4\\cdot{10000} + 0.2\\cdot{20000} -0.1\\cdot{8000} + 0.3\\cdot{5000})\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "so now we can also Expected-Profit is written as."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "{E}_{x}[P] = \\sum_{i={A,B,C,D}}P(X=i)\\cdot{G(X=i)}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 .Information Content (IC)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's take an example and try to understand this concept."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "suppose that the direction of sunrise is what, and we have some random variabels of direction. ${E}$,${W}$,${S}$,${N}$ and asked by some people then definetly we got the answer as probability is ${E}$ that is (1). so you can say that is  there no more information Content(gain) by this example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's some other example , suppose that today is strome, so after listen this we are getting surprised ohh today is strome that mean, the Information Content(gain) is More because on daily basis there in no strome."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "so we can say that"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "storme is a kind of random variabel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "IC(X=strome) \\propto {surprised}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And when we talk about the ${Probability}$ then if there is ${Surprised}$ event that mean they have lower probability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "IC(X=strome) \\propto \\frac{1}{P(X=strome)}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And lower ${Probability}$ that mean you gain more ${Information}$ by knowing about it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "so here ${IC}$ is function  of the ${Probability}$ of that event, but at this movement we cannot explain what is the function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "IC(P(X=strome))\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's take an another example to know more about the ${Information- Content}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have a ${X}$ random variables of team is${[A,B,C,D]}$ and another ${Y}$ random variables of AC is ${[{on} ,{off}]}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's say that team B is win the match and AC is (on)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "so we can say that these two random variabels are independent to each other that mean, if team ${B}$ is win the match is does not matter the AC is ${on}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "so ${Information -Content}$ of two independent random variables is the sum of  information gain by individual random variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "IC(X=B\\cap{Y}=on) = IC(X=B) + IC(Y=on)\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and i told you that in ${IC}$ is depend on  ${probability}$ of that event."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "so now if there is two random varibales are independent to each other then {IC} is written as."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "IC(P(X\\cap{Y})) = IC(P(X)) +IC(P(Y))\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can say that ${Information-content(gain)}$ of an event is proportional to the probability of the event."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "IC(P(X)\\cdot{P(Y)}) = IC(P(X)) + IC(P(Y))\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am going to introduced some basic concept is that we want to know the function which keep properties like above expression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "f(a\\cdot{b}) = f(a)+f(b)\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this properties is looing in the ${\\log}$ function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "{\\log(a\\cdot{b})} = {\\log(a)} + {\\log(b)}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we know in advanced."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "IC(X=A) = {\\frac{1}{P(X=A)}}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and we function used is ${\\log}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "IC(X=A) = {\\log{\\frac{1}{P(X=A)}}}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\\begin{equation}\n",
    "IC(X=A) =  {\\log1}-{\\log{P(X=A)}}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we considering base is ${2}$.\n",
    "\\begin{equation}\n",
    "IC(X=A) = -{\\log_{2}P(X=A)}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 . Entropy Of a Random Variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's start."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have a random varibales of X is${[A,B,C,D]}$ and ${Probability}$ associated these random variables is ${[P(X=A),P(X=B),P(X=C),P(X=D)]}$ and corresponding ${IC}$ is ${[-log_{2}P(X=A),-log_{2}P(X=B),-log_{2}P(X=C),-log_{2}P(X=D)]}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now our ${Expected-Gain}$ is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "E[Gain] = \\sum_{i={A,B,C,D}}P(X=i)\\cdot{Gain(X=i)}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is Entropy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entropy is the Expected Information Content of Random variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and how i am  going to compute that is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "H(X) = {-}\\sum_{i={A,B,C,D}}P(X=i)\\cdot{\\log_{2}P(X=i)}\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.Information Content for Number of Bits Required To Transmit a Massage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's suppose you transmit a Massage to someone which is ${X}$ followed by some command is${A,B,C,D}$. and we know that in digital world we transmit as massage by using bits or bytes and and for this four massage we need to two bit transmit them like ${[00,01,10,11]}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "so now what we doing here we assume some probability distribution of these massage random variables which given by ${[1/4,1/4,1/4,1/4]}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and let's what is the information content of this probability distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "IC(X=A) = {-}{\\log_{2}P(X=A)}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "IC(X=A) = {-}{\\log_{2}{\\frac{1}{4}}}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "IC(X=A) = {-}{\\log_{2}{2^{-2}}}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "IC(X=A) = 2\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this will be same for all massages is ${2}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "From above results you can that No of bit Required to transmit a Massage is equal to Information that Massage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for proving this above concept we take an another example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we have ${8}$ massages like ${[A,B,C,D,E,F,G,H]}$ and ${3}$ bits required to transmit this massages is${[000,001,010,011,100,101,110,111]}$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "so now what we doing here we assume some probability distribution of these massage random variables which given by ${[1/8,1/8,1/8,1/8,1/8,1/8,1/8,1/8]}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and let's what is the information content of this probability distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "IC(X=A) = {-}{\\log_{2}P(X=A)}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "IC(X=A) = {-}{\\log_{2}{\\frac{1}{8}}}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "IC(X=A) = {-}{\\log_{2}{2^{-3}}}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "IC(X=A) = 3\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this will be same for all massages is ${3}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's suppose you are continuosly sending massages to your friend and what you are trying to do you minimized the no of bits that bits used to sending the massages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now suppose that you have four massages ${[A,B,C,D]}$ instead of this distribution ${[1/4,1/4,1/4,1/4]}$ you have ${[1/2,1/4,1/8,1/8]}$ this distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if we want to send A massage more Frequently, the we try to used less no of bits that on Average bits are less."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "so in this case our Information Content(gain) is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "IC(X=A) = {-}{\\log_{2}{\\frac{1}{2}}} = 1\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "IC(X=A) = {-}{\\log_{2}{\\frac{1}{4}}} = 2\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "IC(X=A) = {-}{\\log_{2}{\\frac{1}{8}}} = 3\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "IC(X=A) = {-}{\\log_{2}{\\frac{1}{8}}} = 3\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "you can see that more frequent massages having less no of bits and less frequent massage having the more no bits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "so that at an average no of bits is less than ${2}$ which is ture distribution using the ${2}$  bits. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "how we can compute this average no of bits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "question is what is the expected no of bits i will used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have random variables ${[A,B,C,D]}$ and there Probability Distribution is ${[1/2,1/4,1/8,1/8]}$ and corresponding IC is${[1,2,3,3]}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "= \\frac{1}{2}\\cdot{1} + \\frac{1}{4}\\cdot{2} + \\frac{1}{8}\\cdot{3} + \\frac{1}{8}\\cdot{3}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    " = 1.75\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which is less than ${2}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So above calculation is come from this equation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "H(X) = {-}\\sum_{i={A,B,C,D}}P(X=i)\\cdot{\\log_{2}P(X=i)}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this is  an ${Entropy}$ ,  Entropy of the random variables is actually tells us the Average no of bits required to transmit that random variables. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 .Cross Entropy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take an example for understanding the concept of ${Cross-Entropy}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have a Random Variables of X which is four massages ${[A,B,C,D]}$ and they have ${y}$ is True Probability distribution of that random variables is ${[{y}_1,{y}_2,{y}_3,{y}_4]}$ and Information Content (IC)of each of these random varibels is ${[{-\\log{{y}_1}},{-\\log{{y}_2}},{-\\log{{y}_3}},{-\\log{{y}_4}}]}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's take a  scenario where a man which continuosly sending a massages from a particular source to a particular destination. and that massages having some True probability distribution associated with this massages which is given by ${y}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But you don't know in advanced what is the true probability distribution associated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "then what are you doing in this case, you are trying to estimates the probability distribution associated with this massages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which is represented as ${\\hat{y}}$ is random variables of this estimates ${[{\\hat{y}_1},{\\hat{y}_2},{\\hat{y}_3},{\\hat{y}_4}]}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and we the ${True-Entropy}$ of this random variables are."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    " = -\\sum_{i=A,B,C,D}y_i{\\log{y_i}}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "you are estimates this probability of this massages then Information content ${IC}$ is ${[{-\\log{\\hat{y_1}}},{-\\log{\\hat{y_2}}},{-\\log{\\hat{y_3}}},{-\\log{\\hat{y_4}}}]}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "so now our actuall no of bits getting from this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    " = -\\sum_{i=A,B,C,D}{\\hat{y}_i}{\\log{y_i}}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "instead of this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    " = -\\sum_{i=A,B,C,D}{{y}_i}{\\log{\\hat{y}_i}}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "because the Values associated with this massages is ${[-{\\log{\\hat{y}_i}}]}$. but our actuall massages are come from this true distribution ${[{y}_1,{y}_2,{y}_3,{y}_4]}$ which is you don't know, you are estimated only.\n",
    "that is why we are using this formula."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and this is your ${Cross-Entropy}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "=-\\sum_{i=A,B,C,D}{y}_i{\\log{\\hat{y}_i}}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and based  on this concept we are going to discuss the ${KL-Divergance}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 . KL-Divergence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you know the True distribution then no of bits required would have been."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "=-\\sum{y}_i{\\log{y_i}}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "so we can says that the above expression is ${Self-Entropy}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "but when you don't know the distribution then no of bits required is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "=-\\sum{{y}_i}{\\log{\\hat{y}_i}}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and this expression we can says that ${Cross-Entropy}$. Because in this case we are using two probability distribution for finding the no of bits required to transmit  massages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "so base of these two quantity we are trying find the difference between two distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I want want to find the distance between ${y||{\\hat{y}}}$ and i can used the difference between no of bits in ${Cross-Entropy }$ and no of bits in ${Self-Entropy}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Difference Between these two ${y}$ and ${\\hat{y}}$ is called the ${KL-Divergence}$ and is given by."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "KL(y||\\hat{y}) = (-\\sum{{y}_i}{\\log{\\hat{y}_i}}) + (-\\sum{{y}_i}{\\log{{y}_i}})\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and how we connect this classification problem which i tell you in Multi-class-logistic-regression which is i am going  make detailed explaination on this concept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
